# ---------------------------
# LLM Provider API Keys
# ---------------------------

# Anthropic (Claude 3 family: claude-3-5-sonnet, claude-3-opus, claude-3-5-haiku)
# Get from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-api-key

# DeepSeek (deepseek-chat, deepseek-reasoner, etc.)
# Get from: https://platform.deepseek.com/
DEEPSEEK_API_KEY=your-deepseek-api-key

# Groq (runs models like LLaMA3, DeepSeek, etc.)
# Get from: https://console.groq.com/
GROQ_API_KEY=your-groq-api-key

# Google Gemini (gemini-2.5-flash, gemini-2.5-pro)
# Get from: https://ai.google.dev/
GOOGLE_API_KEY=your-google-api-key

# OpenAI (gpt-4o, gpt-4o-mini, gpt-4.1, etc.)
# Get from: https://platform.openai.com/
OPENAI_API_KEY=your-openai-api-key
# Optional: custom base URL (for Azure OpenAI or proxy)
# OPENAI_API_BASE=https://api.openai.com/v1

# Ollama (local inference: llama2, mistral, phi3, etc.)
# Default assumes local docker/desktop installation
OLLAMA_HOST=localhost
OLLAMA_BASE_URL=http://localhost:11434


# ---------------------------
# Database
# ---------------------------

# SQLite (local dev)
DATABASE_URL=sqlite:///./fingpt.db

# Postgres (production / cloud)
# DATABASE_URL=postgresql+psycopg://username:password@hostname:5432/dbname

# Log SQL statements (set to "1" for verbose debugging)
SQL_ECHO=0


# ---------------------------
# Runtime / Misc
# ---------------------------

# Maximum number of concurrent summarizer tasks
CONCURRENCY=6

# Default batch size of articles to summarize
BATCH_LIMIT=100

# Path to config.yaml (can override at runtime with MNS_CONFIG)
MNS_CONFIG=config/config.yaml
